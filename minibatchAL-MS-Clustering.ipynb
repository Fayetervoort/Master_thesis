{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from time import time\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torcheval.metrics import MulticlassPrecision\n",
    "from torcheval.metrics.classification import MulticlassRecall\n",
    "from torcheval.metrics import MulticlassF1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the amount of labels\n",
    "label_num = 100\n",
    "# set maximum separation setting to True or False\n",
    "ms = True\n",
    "# Adjust number of labels for maximum separation\n",
    "if ms == True:\n",
    "    label_num = label_num - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(label_num)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use GPU if available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:h6ypsjf2) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fff88261ee84cbf813feae97a1d9cf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>custom_step</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>test/accuracy</td><td>▁▂▄▅▅▆▆▇▇█</td></tr><tr><td>test_f1</td><td>▁▃▄▅▆▆▆▇▇█</td></tr><tr><td>test_precision</td><td>▁▃▄▅▆▆▆▇▇█</td></tr><tr><td>test_recall</td><td>▁▂▄▅▅▆▆▇▇█</td></tr><tr><td>train/batchwise_loss</td><td>█▆▄▂▇▁▁▁▂▁▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/epoch</td><td>▁▃▅▇▁▃▅▇▁▃▅▇▂▃▅▇▁▄▆▇▂▃▅█▂▄▆▇▂▄▆█▂▄▆█▂▄▆█</td></tr><tr><td>train/lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/weight_decay</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>custom_step</td><td>10</td></tr><tr><td>test/accuracy</td><td>0.1941</td></tr><tr><td>test_f1</td><td>0.19336</td></tr><tr><td>test_precision</td><td>0.20019</td></tr><tr><td>test_recall</td><td>0.1941</td></tr><tr><td>train/batchwise_loss</td><td>3e-05</td></tr><tr><td>train/epoch</td><td>49.99776</td></tr><tr><td>train/lr</td><td>0.001</td></tr><tr><td>train/weight_decay</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Maximum-Separation-Clust</strong> at: <a href='https://wandb.ai/faye-tervoort/CIFAR-100/runs/h6ypsjf2' target=\"_blank\">https://wandb.ai/faye-tervoort/CIFAR-100/runs/h6ypsjf2</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230630_134807-h6ypsjf2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:h6ypsjf2). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c3ca36f08ad43f189dfae5395a27b76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016667879198212176, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/gpfs/home5/tervoort/wandb/run-20230630_144513-x3e6ztbd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/faye-tervoort/CIFAR-100/runs/x3e6ztbd' target=\"_blank\">Maximum-Separation-Clust</a></strong> to <a href='https://wandb.ai/faye-tervoort/CIFAR-100' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/faye-tervoort/CIFAR-100' target=\"_blank\">https://wandb.ai/faye-tervoort/CIFAR-100</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/faye-tervoort/CIFAR-100/runs/x3e6ztbd' target=\"_blank\">https://wandb.ai/faye-tervoort/CIFAR-100/runs/x3e6ztbd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<wandb.sdk.wandb_metric.Metric at 0x150a3ce4bc10>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_name = \"CIFAR-100\"\n",
    "run_name = \"Maximum-Separation-Clust\"\n",
    "wandb.init(project=project_name, name=run_name,settings=wandb.Settings(start_method='fork'))\n",
    "wandb.define_metric(\"custom_step\")\n",
    "wandb.define_metric(\n",
    "  \"test/accuracy\", step_metric=\"custom_step\")\n",
    "wandb.define_metric(\n",
    "  \"test_precision\", step_metric=\"custom_step\")\n",
    "wandb.define_metric(\n",
    "  \"test_recall\", step_metric=\"custom_step\")\n",
    "wandb.define_metric(\n",
    "  \"test_f1\", step_metric=\"custom_step\")\n",
    "\n",
    "# wandb.config.update(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# import the CIFAR-10 dataset\n",
    "#train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "#test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# import the CIFAR-100 dataset\n",
    "train_set = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_set = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# import the SVHN dataset\n",
    "#train_set = torchvision.datasets.SVHN(root='./data', split='train', download=True, transform=transforms.ToTensor())\n",
    "#test_set = torchvision.datasets.SVHN(root='./data', split='test', download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, num_classes=label_num):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(32*32*3, 100) # Fully connected layer with 100 hidden neurons\n",
    "        self.fc2 = nn.Linear(100, num_classes) # Fully connected layer with num_classes outputs\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 32*32*3) # reshape the input tensor\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"resnet in pytorch\n",
    "\n",
    "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun.\n",
    "\n",
    "    Deep Residual Learning for Image Recognition\n",
    "    https://arxiv.org/abs/1512.03385v1\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"Basic Block for resnet 18 and resnet 34\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #BasicBlock and BottleNeck block\n",
    "    #have different output size\n",
    "    #we use class attribute expansion\n",
    "    #to distinct\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        #residual function\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels * BasicBlock.expansion, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
    "        )\n",
    "\n",
    "        #shortcut\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        #the shortcut output dimension is not the same with residual function\n",
    "        #use 1*1 convolution to match the dimension\n",
    "        if stride != 1 or in_channels != BasicBlock.expansion * out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * BasicBlock.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.ReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))\n",
    "\n",
    "class BottleNeck(nn.Module):\n",
    "    \"\"\"Residual block for resnet over 50 layers\n",
    "\n",
    "    \"\"\"\n",
    "    expansion = 4\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, stride=stride, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n",
    "        )\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * BottleNeck.expansion, stride=stride, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * BottleNeck.expansion)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.ReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, num_block, num_classes=100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True))\n",
    "        #we use a different inputsize than the original paper\n",
    "        #so conv2_x's stride is 1\n",
    "        self.conv2_x = self._make_layer(block, 64, num_block[0], 1)\n",
    "        self.conv3_x = self._make_layer(block, 128, num_block[1], 2)\n",
    "        self.conv4_x = self._make_layer(block, 256, num_block[2], 2)\n",
    "        self.conv5_x = self._make_layer(block, 512, num_block[3], 2)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        \"\"\"make resnet layers(by layer i didnt mean this 'layer' was the\n",
    "        same as a neuron netowork layer, ex. conv layer), one layer may\n",
    "        contain more than one residual block\n",
    "\n",
    "        Args:\n",
    "            block: block type, basic block or bottle neck block\n",
    "            out_channels: output depth channel number of this layer\n",
    "            num_blocks: how many blocks per layer\n",
    "            stride: the stride of the first block of this layer\n",
    "\n",
    "        Return:\n",
    "            return a resnet layer\n",
    "        \"\"\"\n",
    "\n",
    "        # we have num_block blocks per layer, the first block\n",
    "        # could be 1 or 2, other blocks would always be 1\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.conv1(x)\n",
    "        output = self.conv2_x(output)\n",
    "        output = self.conv3_x(output)\n",
    "        output = self.conv4_x(output)\n",
    "        output = self.conv5_x(output)\n",
    "        output = self.avg_pool(output)\n",
    "        output = output.view(output.size(0), -1)\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "def resnet18():\n",
    "    \"\"\" return a ResNet 18 object\n",
    "    \"\"\"\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "def resnet34(dims):\n",
    "    \"\"\" return a ResNet 34 object\n",
    "    \"\"\"\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes=dims)\n",
    "\n",
    "def resnet50(dims):\n",
    "    \"\"\" return a ResNet 50 object\n",
    "    \"\"\"\n",
    "    return ResNet(BottleNeck, [3, 4, 6, 3], num_classes=dims)\n",
    "\n",
    "def resnet101(dims):\n",
    "    \"\"\" return a ResNet 101 object\n",
    "    \"\"\"\n",
    "    return ResNet(BottleNeck, [3, 4, 23, 3], num_classes=dims)\n",
    "\n",
    "def resnet152():\n",
    "    \"\"\" return a ResNet 152 object\n",
    "    \"\"\"\n",
    "    return ResNet(BottleNeck, [3, 8, 36, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = resnet34(label_num)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.setrecursionlimit(10000) #for nr_prototypes>=1000\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prototypes(nr_prototypes):\n",
    "    assert nr_prototypes > 0\n",
    "    prototypes = V(nr_prototypes - 1).T\n",
    "    assert prototypes.shape == (nr_prototypes, nr_prototypes - 1)\n",
    "    assert np.all(np.abs(np.sum(np.power(prototypes, 2), axis=1) - 1) <= 1e-6)\n",
    "    distances = cdist(prototypes, prototypes)\n",
    "    assert distances[~np.eye(*distances.shape, dtype=bool)].std() <= 1e-3\n",
    "    return prototypes.astype(np.float32)\n",
    "\n",
    "def V(order):\n",
    "    if order == 1:\n",
    "        return np.array([[1, -1]])\n",
    "    else:\n",
    "        col1 = np.zeros((order, 1))\n",
    "        col1[0] = 1\n",
    "        row1 = -1 / order * np.ones((1, order))\n",
    "        return np.concatenate((col1, np.concatenate((row1, np.sqrt(1 - 1 / (order**2)) * V(order - 1)), axis=0)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into PyTorch DataLoader\n",
    "# train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Create prototypes for maximum separation\n",
    "prototypes = create_prototypes(100)\n",
    "prototypes = torch.from_numpy(prototypes).float()\n",
    "prototypes *= 0.1\n",
    "dims = prototypes.shape[1]\n",
    "prototypes = prototypes.t()\n",
    "prototypes = prototypes.to(device)\n",
    "test_prototypes = prototypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(train_loader, label_num, ms):\n",
    "    # train the model\n",
    "    num_epochs = 50\n",
    "    train_loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_loss_history = []\n",
    "    val_acc_history = []\n",
    "    metric_p = MulticlassPrecision(average=\"macro\", num_classes=100)\n",
    "    metric_r = MulticlassRecall(average=\"macro\", num_classes=100)\n",
    "    metric_f1 = MulticlassF1Score(average=\"macro\", num_classes=100)\n",
    "\n",
    "\n",
    "    # Loop through the number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        val_loss = 0.0\n",
    "        val_acc = 0.0\n",
    "        metric_p.reset()\n",
    "        metric_r.reset()\n",
    "        metric_f1.reset()\n",
    "\n",
    "\n",
    "        # set model to train mode\n",
    "        model.train()\n",
    "        n_steps_per_epoch = len(train_loader.dataset) / train_loader.batch_size\n",
    "        # iterate over the training data\n",
    "        for batch_index,(inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.to(device)\n",
    "            # Maximum separation\n",
    "            if ms == True:\n",
    "                outputs = torch.mm(outputs, prototypes)\n",
    "            #compute the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # increment the running loss and accuracy\n",
    "            train_loss += loss.item()\n",
    "            train_acc += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        # calculate the average training loss and accuracy\n",
    "        train_loss /= len(train_loader)\n",
    "        train_loss_history.append(train_loss)\n",
    "        train_acc /= len(train_loader.dataset)\n",
    "        train_acc_history.append(train_acc)\n",
    "\n",
    "        wandb.log({\"train/batchwise_loss\": loss.item(),\n",
    "        \"train/lr\": optimizer.param_groups[0][\"lr\"],\n",
    "        \"train/weight_decay\": optimizer.param_groups[0][\"weight_decay\"],\n",
    "        \"train/epoch\": ((batch_index)/ n_steps_per_epoch + epoch),\n",
    "        })\n",
    "\n",
    "\n",
    "        # set the model to evaluation mode\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                outputs = outputs.to(device)\n",
    "                # Maximum separation\n",
    "                if ms == True:\n",
    "                    outputs = torch.mm(outputs, prototypes)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                val_acc += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "                # update precision metric\n",
    "                metric_p.to(device)\n",
    "                metric_p.update(outputs.argmax(1), labels)\n",
    "\n",
    "                # update recall metric\n",
    "                metric_r.to(device)\n",
    "                metric_r.update(outputs.argmax(1), labels)\n",
    "\n",
    "                # update F1-score metric\n",
    "                metric_f1.to(device)\n",
    "                metric_f1.update(outputs.argmax(1), labels)\n",
    "\n",
    "\n",
    "        # calculate the average validation loss and accuracy\n",
    "        val_loss /= len(test_loader)\n",
    "        val_loss_history.append(val_loss)\n",
    "        val_acc /= len(test_loader.dataset)\n",
    "        val_acc_history.append(val_acc)\n",
    "\n",
    "        # calculate the precision, recall and F1-score\n",
    "        precision = metric_p.compute()\n",
    "        recall = metric_r.compute()\n",
    "        f1 = metric_f1.compute()\n",
    "\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, train loss: {train_loss:.4f}, train acc: {train_acc:.4f}, val loss: {val_loss:.4f}, val acc: {val_acc:.4f}')\n",
    "\n",
    "    return train_acc, val_acc, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diverse mini-Batch Active Learning input\n",
    "import random\n",
    "\n",
    "unlabeled_dataset = train_set\n",
    "batch_size = 1000\n",
    "n_iter = 10\n",
    "#use maximum separation\n",
    "#use logits\n",
    "\n",
    "\n",
    "\n",
    "# Create random k number of images\n",
    "randomlist = random.sample(range(0, 50000), batch_size)\n",
    "run1_randomlist = randomlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "1000\n",
      "49000\n",
      "766\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoader from trainset\n",
    "trainset_loader = torch.utils.data.DataLoader(train_set, shuffle = False, batch_size=64, num_workers=2)\n",
    "print(len(trainset_loader.dataset))\n",
    "\n",
    "# Create a sampler that selects the first k number of random images\n",
    "train_sampler = torch.utils.data.Subset(trainset_loader.dataset, randomlist)\n",
    "train_loader = torch.utils.data.DataLoader(train_sampler, shuffle = False, batch_size=64, num_workers=2)\n",
    "print(len(train_loader.dataset))\n",
    "\n",
    "# Select the still unlabeled data and create a DataLoader\n",
    "all_data_list = list(range(0, 50000))\n",
    "unlabeled_data_list = [i for i in all_data_list if i not in randomlist]\n",
    "print(len(unlabeled_data_list))\n",
    "unlabeled_sampler = torch.utils.data.Subset(trainset_loader.dataset, unlabeled_data_list)\n",
    "unlabeled_loader = torch.utils.data.DataLoader(unlabeled_sampler, shuffle = False, batch_size=64, num_workers=2)\n",
    "print(len(unlabeled_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, train loss: 4.6127, train acc: 0.0150, val loss: 9.2995, val acc: 0.0113\n",
      "Epoch 2/50, train loss: 4.4725, train acc: 0.0230, val loss: 5.7486, val acc: 0.0197\n",
      "Epoch 3/50, train loss: 4.3591, train acc: 0.0370, val loss: 5.2032, val acc: 0.0244\n",
      "Epoch 4/50, train loss: 4.2753, train acc: 0.0400, val loss: 4.6778, val acc: 0.0268\n",
      "Epoch 5/50, train loss: 4.2145, train acc: 0.0400, val loss: 4.6162, val acc: 0.0292\n",
      "Epoch 6/50, train loss: 4.1410, train acc: 0.0450, val loss: 4.4813, val acc: 0.0277\n",
      "Epoch 7/50, train loss: 4.0851, train acc: 0.0480, val loss: 4.3754, val acc: 0.0354\n",
      "Epoch 8/50, train loss: 4.0359, train acc: 0.0590, val loss: 4.5090, val acc: 0.0343\n",
      "Epoch 9/50, train loss: 3.9667, train acc: 0.0660, val loss: 4.4027, val acc: 0.0397\n",
      "Epoch 10/50, train loss: 3.9017, train acc: 0.0810, val loss: 4.3401, val acc: 0.0449\n",
      "Epoch 11/50, train loss: 3.8178, train acc: 0.0840, val loss: 4.3746, val acc: 0.0487\n",
      "Epoch 12/50, train loss: 3.7392, train acc: 0.1030, val loss: 4.3568, val acc: 0.0481\n",
      "Epoch 13/50, train loss: 3.6628, train acc: 0.1060, val loss: 4.3485, val acc: 0.0480\n",
      "Epoch 14/50, train loss: 3.6130, train acc: 0.1080, val loss: 4.5390, val acc: 0.0299\n",
      "Epoch 15/50, train loss: 3.5853, train acc: 0.1220, val loss: 4.4445, val acc: 0.0477\n",
      "Epoch 16/50, train loss: 3.4927, train acc: 0.1320, val loss: 4.2865, val acc: 0.0596\n",
      "Epoch 17/50, train loss: 3.4248, train acc: 0.1400, val loss: 4.5811, val acc: 0.0479\n",
      "Epoch 18/50, train loss: 3.3352, train acc: 0.1600, val loss: 4.4071, val acc: 0.0652\n",
      "Epoch 19/50, train loss: 3.2683, train acc: 0.1740, val loss: 4.3165, val acc: 0.0685\n",
      "Epoch 20/50, train loss: 3.1584, train acc: 0.1990, val loss: 4.2411, val acc: 0.0758\n",
      "Epoch 21/50, train loss: 3.1015, train acc: 0.2120, val loss: 4.5116, val acc: 0.0599\n",
      "Epoch 22/50, train loss: 3.0375, train acc: 0.2200, val loss: 4.4210, val acc: 0.0714\n",
      "Epoch 23/50, train loss: 2.9978, train acc: 0.2350, val loss: 4.5614, val acc: 0.0666\n",
      "Epoch 24/50, train loss: 2.9769, train acc: 0.2230, val loss: 4.5109, val acc: 0.0686\n",
      "Epoch 25/50, train loss: 2.8536, train acc: 0.2620, val loss: 4.6511, val acc: 0.0643\n",
      "Epoch 26/50, train loss: 2.7829, train acc: 0.2700, val loss: 4.4211, val acc: 0.0752\n",
      "Epoch 27/50, train loss: 2.6441, train acc: 0.3030, val loss: 4.3696, val acc: 0.0767\n",
      "Epoch 28/50, train loss: 2.5013, train acc: 0.3380, val loss: 4.4421, val acc: 0.0733\n",
      "Epoch 29/50, train loss: 2.3413, train acc: 0.3810, val loss: 4.6284, val acc: 0.0723\n",
      "Epoch 30/50, train loss: 2.1707, train acc: 0.4530, val loss: 4.5323, val acc: 0.0751\n",
      "Epoch 31/50, train loss: 2.0647, train acc: 0.4770, val loss: 4.6457, val acc: 0.0697\n",
      "Epoch 32/50, train loss: 1.9371, train acc: 0.5110, val loss: 4.6792, val acc: 0.0770\n",
      "Epoch 33/50, train loss: 1.8078, train acc: 0.5540, val loss: 4.7071, val acc: 0.0699\n",
      "Epoch 34/50, train loss: 1.6892, train acc: 0.5790, val loss: 4.7991, val acc: 0.0658\n",
      "Epoch 35/50, train loss: 1.6602, train acc: 0.5790, val loss: 5.0586, val acc: 0.0681\n",
      "Epoch 36/50, train loss: 1.6342, train acc: 0.5980, val loss: 5.1685, val acc: 0.0725\n",
      "Epoch 37/50, train loss: 1.6209, train acc: 0.6030, val loss: 5.2933, val acc: 0.0630\n",
      "Epoch 38/50, train loss: 1.4979, train acc: 0.6280, val loss: 5.1887, val acc: 0.0679\n",
      "Epoch 39/50, train loss: 1.2546, train acc: 0.7060, val loss: 4.9430, val acc: 0.0822\n",
      "Epoch 40/50, train loss: 1.0931, train acc: 0.7660, val loss: 5.0033, val acc: 0.0796\n",
      "Epoch 41/50, train loss: 0.8922, train acc: 0.8180, val loss: 4.9954, val acc: 0.0747\n",
      "Epoch 42/50, train loss: 0.6893, train acc: 0.8830, val loss: 4.9424, val acc: 0.0892\n",
      "Epoch 43/50, train loss: 0.5366, train acc: 0.9190, val loss: 5.1319, val acc: 0.0863\n",
      "Epoch 44/50, train loss: 0.4369, train acc: 0.9390, val loss: 5.1435, val acc: 0.0877\n",
      "Epoch 45/50, train loss: 0.3582, train acc: 0.9480, val loss: 5.1414, val acc: 0.0826\n",
      "Epoch 46/50, train loss: 0.3097, train acc: 0.9590, val loss: 5.2182, val acc: 0.0800\n",
      "Epoch 47/50, train loss: 0.2568, train acc: 0.9630, val loss: 5.2077, val acc: 0.0835\n",
      "Epoch 48/50, train loss: 0.2153, train acc: 0.9750, val loss: 5.2665, val acc: 0.0836\n",
      "Epoch 49/50, train loss: 0.1805, train acc: 0.9800, val loss: 5.3527, val acc: 0.0881\n",
      "Epoch 50/50, train loss: 0.1436, train acc: 0.9860, val loss: 5.2816, val acc: 0.0900\n",
      "Epoch 1/50, train loss: 2.5486, train acc: 0.5529, val loss: 6.4152, val acc: 0.0350\n",
      "Epoch 2/50, train loss: 3.4215, train acc: 0.1772, val loss: 4.2956, val acc: 0.0777\n",
      "Epoch 3/50, train loss: 2.5655, train acc: 0.3429, val loss: 4.3889, val acc: 0.0845\n",
      "Epoch 4/50, train loss: 1.8787, train acc: 0.5399, val loss: 4.8159, val acc: 0.0766\n",
      "Epoch 5/50, train loss: 1.3881, train acc: 0.6853, val loss: 5.3098, val acc: 0.0629\n",
      "Epoch 6/50, train loss: 1.1396, train acc: 0.7499, val loss: 5.9148, val acc: 0.0525\n",
      "Epoch 7/50, train loss: 1.1322, train acc: 0.7243, val loss: 5.2572, val acc: 0.0797\n",
      "Epoch 8/50, train loss: 1.0319, train acc: 0.7389, val loss: 5.3609, val acc: 0.0822\n",
      "Epoch 9/50, train loss: 0.6918, train acc: 0.8525, val loss: 5.4063, val acc: 0.0768\n",
      "Epoch 10/50, train loss: 0.4638, train acc: 0.9208, val loss: 5.2654, val acc: 0.0951\n",
      "Epoch 11/50, train loss: 0.3480, train acc: 0.9396, val loss: 5.4831, val acc: 0.0879\n",
      "Epoch 12/50, train loss: 0.2566, train acc: 0.9646, val loss: 5.3861, val acc: 0.0976\n",
      "Epoch 13/50, train loss: 0.1570, train acc: 0.9797, val loss: 5.3808, val acc: 0.1011\n",
      "Epoch 14/50, train loss: 0.1043, train acc: 0.9891, val loss: 5.3785, val acc: 0.1123\n",
      "Epoch 15/50, train loss: 0.0604, train acc: 0.9906, val loss: 5.2431, val acc: 0.1141\n",
      "Epoch 16/50, train loss: 0.0461, train acc: 0.9948, val loss: 5.2080, val acc: 0.1102\n",
      "Epoch 17/50, train loss: 0.0357, train acc: 0.9953, val loss: 5.2322, val acc: 0.1155\n",
      "Epoch 18/50, train loss: 0.0368, train acc: 0.9943, val loss: 5.2549, val acc: 0.1169\n",
      "Epoch 19/50, train loss: 0.0245, train acc: 0.9974, val loss: 5.2204, val acc: 0.1116\n",
      "Epoch 20/50, train loss: 0.0178, train acc: 0.9995, val loss: 5.2033, val acc: 0.1136\n",
      "Epoch 21/50, train loss: 0.0133, train acc: 1.0000, val loss: 5.2421, val acc: 0.1132\n",
      "Epoch 22/50, train loss: 0.0114, train acc: 1.0000, val loss: 5.2371, val acc: 0.1146\n",
      "Epoch 23/50, train loss: 0.0097, train acc: 1.0000, val loss: 5.2449, val acc: 0.1152\n",
      "Epoch 24/50, train loss: 0.0088, train acc: 1.0000, val loss: 5.2485, val acc: 0.1148\n",
      "Epoch 25/50, train loss: 0.0082, train acc: 1.0000, val loss: 5.2554, val acc: 0.1150\n",
      "Epoch 26/50, train loss: 0.0076, train acc: 1.0000, val loss: 5.2634, val acc: 0.1149\n",
      "Epoch 27/50, train loss: 0.0071, train acc: 1.0000, val loss: 5.2712, val acc: 0.1144\n",
      "Epoch 28/50, train loss: 0.0067, train acc: 1.0000, val loss: 5.2789, val acc: 0.1150\n",
      "Epoch 29/50, train loss: 0.0064, train acc: 1.0000, val loss: 5.2865, val acc: 0.1156\n",
      "Epoch 30/50, train loss: 0.0060, train acc: 1.0000, val loss: 5.2941, val acc: 0.1155\n",
      "Epoch 31/50, train loss: 0.0057, train acc: 1.0000, val loss: 5.3014, val acc: 0.1157\n",
      "Epoch 32/50, train loss: 0.0054, train acc: 1.0000, val loss: 5.3086, val acc: 0.1156\n",
      "Epoch 33/50, train loss: 0.0052, train acc: 1.0000, val loss: 5.3155, val acc: 0.1157\n",
      "Epoch 34/50, train loss: 0.0049, train acc: 1.0000, val loss: 5.3225, val acc: 0.1158\n",
      "Epoch 35/50, train loss: 0.0047, train acc: 1.0000, val loss: 5.3292, val acc: 0.1162\n",
      "Epoch 36/50, train loss: 0.0045, train acc: 1.0000, val loss: 5.3358, val acc: 0.1163\n",
      "Epoch 37/50, train loss: 0.0043, train acc: 1.0000, val loss: 5.3424, val acc: 0.1160\n",
      "Epoch 38/50, train loss: 0.0042, train acc: 1.0000, val loss: 5.3489, val acc: 0.1163\n",
      "Epoch 39/50, train loss: 0.0040, train acc: 1.0000, val loss: 5.3552, val acc: 0.1157\n",
      "Epoch 40/50, train loss: 0.0038, train acc: 1.0000, val loss: 5.3615, val acc: 0.1158\n",
      "Epoch 41/50, train loss: 0.0037, train acc: 1.0000, val loss: 5.3676, val acc: 0.1159\n",
      "Epoch 42/50, train loss: 0.0036, train acc: 1.0000, val loss: 5.3736, val acc: 0.1161\n",
      "Epoch 43/50, train loss: 0.0034, train acc: 1.0000, val loss: 5.3796, val acc: 0.1160\n",
      "Epoch 44/50, train loss: 0.0033, train acc: 1.0000, val loss: 5.3854, val acc: 0.1157\n",
      "Epoch 45/50, train loss: 0.0032, train acc: 1.0000, val loss: 5.3913, val acc: 0.1160\n",
      "Epoch 46/50, train loss: 0.0031, train acc: 1.0000, val loss: 5.3970, val acc: 0.1162\n",
      "Epoch 47/50, train loss: 0.0030, train acc: 1.0000, val loss: 5.4027, val acc: 0.1161\n",
      "Epoch 48/50, train loss: 0.0029, train acc: 1.0000, val loss: 5.4082, val acc: 0.1163\n",
      "Epoch 49/50, train loss: 0.0028, train acc: 1.0000, val loss: 5.4138, val acc: 0.1161\n",
      "Epoch 50/50, train loss: 0.0027, train acc: 1.0000, val loss: 5.4192, val acc: 0.1161\n",
      "Epoch 1/50, train loss: 1.9004, train acc: 0.6992, val loss: 10.9369, val acc: 0.0371\n",
      "Epoch 2/50, train loss: 3.4926, train acc: 0.1695, val loss: 4.0088, val acc: 0.1013\n",
      "Epoch 3/50, train loss: 2.3509, train acc: 0.3918, val loss: 4.3080, val acc: 0.0990\n",
      "Epoch 4/50, train loss: 1.3107, train acc: 0.6625, val loss: 5.0716, val acc: 0.0887\n",
      "Epoch 5/50, train loss: 0.8273, train acc: 0.7962, val loss: 5.3493, val acc: 0.0905\n",
      "Epoch 6/50, train loss: 0.6120, train acc: 0.8445, val loss: 5.4456, val acc: 0.0925\n",
      "Epoch 7/50, train loss: 0.4557, train acc: 0.8908, val loss: 5.7693, val acc: 0.1038\n",
      "Epoch 8/50, train loss: 0.3211, train acc: 0.9233, val loss: 5.8061, val acc: 0.0982\n",
      "Epoch 9/50, train loss: 0.1360, train acc: 0.9769, val loss: 5.5052, val acc: 0.1115\n",
      "Epoch 10/50, train loss: 0.0581, train acc: 0.9944, val loss: 5.2887, val acc: 0.1324\n",
      "Epoch 11/50, train loss: 0.0215, train acc: 0.9989, val loss: 5.1775, val acc: 0.1366\n",
      "Epoch 12/50, train loss: 0.0091, train acc: 1.0000, val loss: 5.1769, val acc: 0.1410\n",
      "Epoch 13/50, train loss: 0.0069, train acc: 1.0000, val loss: 5.1862, val acc: 0.1423\n",
      "Epoch 14/50, train loss: 0.0055, train acc: 1.0000, val loss: 5.1938, val acc: 0.1413\n",
      "Epoch 15/50, train loss: 0.0048, train acc: 1.0000, val loss: 5.2048, val acc: 0.1413\n",
      "Epoch 16/50, train loss: 0.0044, train acc: 1.0000, val loss: 5.2153, val acc: 0.1406\n",
      "Epoch 17/50, train loss: 0.0040, train acc: 1.0000, val loss: 5.2252, val acc: 0.1405\n",
      "Epoch 18/50, train loss: 0.0037, train acc: 1.0000, val loss: 5.2346, val acc: 0.1405\n",
      "Epoch 19/50, train loss: 0.0035, train acc: 1.0000, val loss: 5.2437, val acc: 0.1407\n",
      "Epoch 20/50, train loss: 0.0032, train acc: 1.0000, val loss: 5.2526, val acc: 0.1403\n",
      "Epoch 21/50, train loss: 0.0030, train acc: 1.0000, val loss: 5.2611, val acc: 0.1406\n",
      "Epoch 22/50, train loss: 0.0029, train acc: 1.0000, val loss: 5.2694, val acc: 0.1400\n",
      "Epoch 23/50, train loss: 0.0027, train acc: 1.0000, val loss: 5.2775, val acc: 0.1394\n",
      "Epoch 24/50, train loss: 0.0026, train acc: 1.0000, val loss: 5.2854, val acc: 0.1396\n",
      "Epoch 25/50, train loss: 0.0024, train acc: 1.0000, val loss: 5.2931, val acc: 0.1398\n",
      "Epoch 26/50, train loss: 0.0023, train acc: 1.0000, val loss: 5.3006, val acc: 0.1396\n",
      "Epoch 27/50, train loss: 0.0022, train acc: 1.0000, val loss: 5.3080, val acc: 0.1397\n",
      "Epoch 28/50, train loss: 0.0021, train acc: 1.0000, val loss: 5.3152, val acc: 0.1397\n",
      "Epoch 29/50, train loss: 0.0020, train acc: 1.0000, val loss: 5.3224, val acc: 0.1394\n",
      "Epoch 30/50, train loss: 0.0019, train acc: 1.0000, val loss: 5.3293, val acc: 0.1396\n",
      "Epoch 31/50, train loss: 0.0018, train acc: 1.0000, val loss: 5.3362, val acc: 0.1393\n",
      "Epoch 32/50, train loss: 0.0018, train acc: 1.0000, val loss: 5.3430, val acc: 0.1391\n",
      "Epoch 33/50, train loss: 0.0017, train acc: 1.0000, val loss: 5.3496, val acc: 0.1390\n",
      "Epoch 34/50, train loss: 0.0016, train acc: 1.0000, val loss: 5.3562, val acc: 0.1389\n",
      "Epoch 35/50, train loss: 0.0016, train acc: 1.0000, val loss: 5.3626, val acc: 0.1387\n",
      "Epoch 36/50, train loss: 0.0015, train acc: 1.0000, val loss: 5.3690, val acc: 0.1385\n",
      "Epoch 37/50, train loss: 0.0015, train acc: 1.0000, val loss: 5.3753, val acc: 0.1384\n",
      "Epoch 38/50, train loss: 0.0014, train acc: 1.0000, val loss: 5.3815, val acc: 0.1383\n",
      "Epoch 39/50, train loss: 0.0013, train acc: 1.0000, val loss: 5.3877, val acc: 0.1384\n",
      "Epoch 40/50, train loss: 0.0013, train acc: 1.0000, val loss: 5.3938, val acc: 0.1382\n",
      "Epoch 41/50, train loss: 0.0013, train acc: 1.0000, val loss: 5.3998, val acc: 0.1378\n",
      "Epoch 42/50, train loss: 0.0012, train acc: 1.0000, val loss: 5.4058, val acc: 0.1380\n",
      "Epoch 43/50, train loss: 0.0012, train acc: 1.0000, val loss: 5.4118, val acc: 0.1380\n",
      "Epoch 44/50, train loss: 0.0011, train acc: 1.0000, val loss: 5.4177, val acc: 0.1381\n",
      "Epoch 45/50, train loss: 0.0011, train acc: 1.0000, val loss: 5.4235, val acc: 0.1380\n",
      "Epoch 46/50, train loss: 0.0011, train acc: 1.0000, val loss: 5.4293, val acc: 0.1383\n",
      "Epoch 47/50, train loss: 0.0010, train acc: 1.0000, val loss: 5.4350, val acc: 0.1382\n",
      "Epoch 48/50, train loss: 0.0010, train acc: 1.0000, val loss: 5.4407, val acc: 0.1383\n",
      "Epoch 49/50, train loss: 0.0010, train acc: 1.0000, val loss: 5.4464, val acc: 0.1385\n",
      "Epoch 50/50, train loss: 0.0009, train acc: 1.0000, val loss: 5.4520, val acc: 0.1382\n",
      "Epoch 1/50, train loss: 1.4248, train acc: 0.7903, val loss: 8.6182, val acc: 0.0513\n",
      "Epoch 2/50, train loss: 2.9002, train acc: 0.2885, val loss: 4.2451, val acc: 0.1173\n",
      "Epoch 3/50, train loss: 1.2059, train acc: 0.6833, val loss: 5.2671, val acc: 0.1076\n",
      "Epoch 4/50, train loss: 0.4488, train acc: 0.8927, val loss: 5.5880, val acc: 0.1173\n",
      "Epoch 5/50, train loss: 0.2123, train acc: 0.9557, val loss: 5.3638, val acc: 0.1255\n",
      "Epoch 6/50, train loss: 0.0915, train acc: 0.9847, val loss: 5.4041, val acc: 0.1310\n",
      "Epoch 7/50, train loss: 0.0398, train acc: 0.9947, val loss: 5.2468, val acc: 0.1470\n",
      "Epoch 8/50, train loss: 0.0183, train acc: 0.9989, val loss: 5.2482, val acc: 0.1472\n",
      "Epoch 9/50, train loss: 0.0057, train acc: 1.0000, val loss: 5.2036, val acc: 0.1498\n",
      "Epoch 10/50, train loss: 0.0038, train acc: 1.0000, val loss: 5.2060, val acc: 0.1501\n",
      "Epoch 11/50, train loss: 0.0032, train acc: 1.0000, val loss: 5.2171, val acc: 0.1513\n",
      "Epoch 12/50, train loss: 0.0029, train acc: 1.0000, val loss: 5.2285, val acc: 0.1513\n",
      "Epoch 13/50, train loss: 0.0026, train acc: 1.0000, val loss: 5.2400, val acc: 0.1522\n",
      "Epoch 14/50, train loss: 0.0024, train acc: 1.0000, val loss: 5.2512, val acc: 0.1521\n",
      "Epoch 15/50, train loss: 0.0022, train acc: 1.0000, val loss: 5.2621, val acc: 0.1523\n",
      "Epoch 16/50, train loss: 0.0020, train acc: 1.0000, val loss: 5.2729, val acc: 0.1525\n",
      "Epoch 17/50, train loss: 0.0019, train acc: 1.0000, val loss: 5.2833, val acc: 0.1532\n",
      "Epoch 18/50, train loss: 0.0018, train acc: 1.0000, val loss: 5.2934, val acc: 0.1539\n",
      "Epoch 19/50, train loss: 0.0017, train acc: 1.0000, val loss: 5.3035, val acc: 0.1546\n",
      "Epoch 20/50, train loss: 0.0016, train acc: 1.0000, val loss: 5.3132, val acc: 0.1547\n",
      "Epoch 21/50, train loss: 0.0015, train acc: 1.0000, val loss: 5.3227, val acc: 0.1552\n",
      "Epoch 22/50, train loss: 0.0014, train acc: 1.0000, val loss: 5.3321, val acc: 0.1559\n",
      "Epoch 23/50, train loss: 0.0013, train acc: 1.0000, val loss: 5.3413, val acc: 0.1563\n",
      "Epoch 24/50, train loss: 0.0012, train acc: 1.0000, val loss: 5.3503, val acc: 0.1564\n",
      "Epoch 25/50, train loss: 0.0012, train acc: 1.0000, val loss: 5.3591, val acc: 0.1561\n",
      "Epoch 26/50, train loss: 0.0011, train acc: 1.0000, val loss: 5.3679, val acc: 0.1561\n",
      "Epoch 27/50, train loss: 0.0011, train acc: 1.0000, val loss: 5.3765, val acc: 0.1564\n",
      "Epoch 28/50, train loss: 0.0010, train acc: 1.0000, val loss: 5.3850, val acc: 0.1562\n",
      "Epoch 29/50, train loss: 0.0010, train acc: 1.0000, val loss: 5.3933, val acc: 0.1565\n",
      "Epoch 30/50, train loss: 0.0009, train acc: 1.0000, val loss: 5.4015, val acc: 0.1567\n",
      "Epoch 31/50, train loss: 0.0009, train acc: 1.0000, val loss: 5.4096, val acc: 0.1567\n",
      "Epoch 32/50, train loss: 0.0009, train acc: 1.0000, val loss: 5.4177, val acc: 0.1568\n",
      "Epoch 33/50, train loss: 0.0008, train acc: 1.0000, val loss: 5.4255, val acc: 0.1567\n",
      "Epoch 34/50, train loss: 0.0008, train acc: 1.0000, val loss: 5.4334, val acc: 0.1570\n",
      "Epoch 35/50, train loss: 0.0008, train acc: 1.0000, val loss: 5.4412, val acc: 0.1571\n",
      "Epoch 36/50, train loss: 0.0007, train acc: 1.0000, val loss: 5.4489, val acc: 0.1569\n",
      "Epoch 37/50, train loss: 0.0007, train acc: 1.0000, val loss: 5.4566, val acc: 0.1567\n",
      "Epoch 38/50, train loss: 0.0007, train acc: 1.0000, val loss: 5.4642, val acc: 0.1565\n",
      "Epoch 39/50, train loss: 0.0006, train acc: 1.0000, val loss: 5.4716, val acc: 0.1568\n",
      "Epoch 40/50, train loss: 0.0006, train acc: 1.0000, val loss: 5.4791, val acc: 0.1571\n",
      "Epoch 41/50, train loss: 0.0006, train acc: 1.0000, val loss: 5.4865, val acc: 0.1573\n",
      "Epoch 42/50, train loss: 0.0006, train acc: 1.0000, val loss: 5.4937, val acc: 0.1571\n",
      "Epoch 43/50, train loss: 0.0005, train acc: 1.0000, val loss: 5.5010, val acc: 0.1574\n",
      "Epoch 44/50, train loss: 0.0005, train acc: 1.0000, val loss: 5.5083, val acc: 0.1573\n",
      "Epoch 45/50, train loss: 0.0005, train acc: 1.0000, val loss: 5.5154, val acc: 0.1571\n",
      "Epoch 46/50, train loss: 0.0005, train acc: 1.0000, val loss: 5.5225, val acc: 0.1570\n",
      "Epoch 47/50, train loss: 0.0005, train acc: 1.0000, val loss: 5.5297, val acc: 0.1572\n",
      "Epoch 48/50, train loss: 0.0005, train acc: 1.0000, val loss: 5.5367, val acc: 0.1574\n",
      "Epoch 49/50, train loss: 0.0004, train acc: 1.0000, val loss: 5.5438, val acc: 0.1577\n",
      "Epoch 50/50, train loss: 0.0004, train acc: 1.0000, val loss: 5.5508, val acc: 0.1580\n",
      "Epoch 1/50, train loss: 1.1798, train acc: 0.8301, val loss: 9.1332, val acc: 0.0523\n",
      "Epoch 2/50, train loss: 2.8051, train acc: 0.3154, val loss: 4.4055, val acc: 0.1149\n",
      "Epoch 3/50, train loss: 0.9373, train acc: 0.7594, val loss: 5.5200, val acc: 0.1039\n",
      "Epoch 4/50, train loss: 0.3050, train acc: 0.9323, val loss: 5.6602, val acc: 0.1140\n",
      "Epoch 5/50, train loss: 0.1258, train acc: 0.9752, val loss: 5.2359, val acc: 0.1456\n",
      "Epoch 6/50, train loss: 0.0585, train acc: 0.9917, val loss: 5.2464, val acc: 0.1531\n",
      "Epoch 7/50, train loss: 0.0246, train acc: 0.9966, val loss: 5.1935, val acc: 0.1615\n",
      "Epoch 8/50, train loss: 0.0084, train acc: 0.9996, val loss: 5.0892, val acc: 0.1681\n",
      "Epoch 9/50, train loss: 0.0035, train acc: 1.0000, val loss: 5.1012, val acc: 0.1703\n",
      "Epoch 10/50, train loss: 0.0026, train acc: 1.0000, val loss: 5.1121, val acc: 0.1696\n",
      "Epoch 11/50, train loss: 0.0022, train acc: 1.0000, val loss: 5.1268, val acc: 0.1702\n",
      "Epoch 12/50, train loss: 0.0019, train acc: 1.0000, val loss: 5.1404, val acc: 0.1699\n",
      "Epoch 13/50, train loss: 0.0017, train acc: 1.0000, val loss: 5.1534, val acc: 0.1700\n",
      "Epoch 14/50, train loss: 0.0016, train acc: 1.0000, val loss: 5.1660, val acc: 0.1699\n",
      "Epoch 15/50, train loss: 0.0014, train acc: 1.0000, val loss: 5.1779, val acc: 0.1703\n",
      "Epoch 16/50, train loss: 0.0013, train acc: 1.0000, val loss: 5.1895, val acc: 0.1709\n",
      "Epoch 17/50, train loss: 0.0012, train acc: 1.0000, val loss: 5.2007, val acc: 0.1712\n",
      "Epoch 18/50, train loss: 0.0012, train acc: 1.0000, val loss: 5.2115, val acc: 0.1713\n",
      "Epoch 19/50, train loss: 0.0011, train acc: 1.0000, val loss: 5.2221, val acc: 0.1712\n",
      "Epoch 20/50, train loss: 0.0010, train acc: 1.0000, val loss: 5.2322, val acc: 0.1709\n",
      "Epoch 21/50, train loss: 0.0010, train acc: 1.0000, val loss: 5.2423, val acc: 0.1711\n",
      "Epoch 22/50, train loss: 0.0009, train acc: 1.0000, val loss: 5.2522, val acc: 0.1714\n",
      "Epoch 23/50, train loss: 0.0009, train acc: 1.0000, val loss: 5.2618, val acc: 0.1711\n",
      "Epoch 24/50, train loss: 0.0008, train acc: 1.0000, val loss: 5.2712, val acc: 0.1716\n",
      "Epoch 25/50, train loss: 0.0008, train acc: 1.0000, val loss: 5.2806, val acc: 0.1720\n",
      "Epoch 26/50, train loss: 0.0007, train acc: 1.0000, val loss: 5.2897, val acc: 0.1721\n",
      "Epoch 27/50, train loss: 0.0007, train acc: 1.0000, val loss: 5.2987, val acc: 0.1721\n",
      "Epoch 28/50, train loss: 0.0007, train acc: 1.0000, val loss: 5.3076, val acc: 0.1719\n",
      "Epoch 29/50, train loss: 0.0006, train acc: 1.0000, val loss: 5.3164, val acc: 0.1717\n",
      "Epoch 30/50, train loss: 0.0006, train acc: 1.0000, val loss: 5.3251, val acc: 0.1716\n",
      "Epoch 31/50, train loss: 0.0006, train acc: 1.0000, val loss: 5.3337, val acc: 0.1714\n",
      "Epoch 32/50, train loss: 0.0005, train acc: 1.0000, val loss: 5.3423, val acc: 0.1718\n",
      "Epoch 33/50, train loss: 0.0005, train acc: 1.0000, val loss: 5.3507, val acc: 0.1716\n",
      "Epoch 34/50, train loss: 0.0005, train acc: 1.0000, val loss: 5.3590, val acc: 0.1712\n",
      "Epoch 35/50, train loss: 0.0005, train acc: 1.0000, val loss: 5.3673, val acc: 0.1711\n",
      "Epoch 36/50, train loss: 0.0004, train acc: 1.0000, val loss: 5.3755, val acc: 0.1712\n",
      "Epoch 37/50, train loss: 0.0004, train acc: 1.0000, val loss: 5.3837, val acc: 0.1714\n",
      "Epoch 38/50, train loss: 0.0004, train acc: 1.0000, val loss: 5.3919, val acc: 0.1715\n",
      "Epoch 39/50, train loss: 0.0004, train acc: 1.0000, val loss: 5.4000, val acc: 0.1711\n",
      "Epoch 40/50, train loss: 0.0004, train acc: 1.0000, val loss: 5.4080, val acc: 0.1712\n",
      "Epoch 41/50, train loss: 0.0004, train acc: 1.0000, val loss: 5.4160, val acc: 0.1712\n",
      "Epoch 42/50, train loss: 0.0003, train acc: 1.0000, val loss: 5.4240, val acc: 0.1717\n",
      "Epoch 43/50, train loss: 0.0003, train acc: 1.0000, val loss: 5.4319, val acc: 0.1718\n",
      "Epoch 44/50, train loss: 0.0003, train acc: 1.0000, val loss: 5.4397, val acc: 0.1718\n",
      "Epoch 45/50, train loss: 0.0003, train acc: 1.0000, val loss: 5.4476, val acc: 0.1719\n",
      "Epoch 46/50, train loss: 0.0003, train acc: 1.0000, val loss: 5.4554, val acc: 0.1719\n",
      "Epoch 47/50, train loss: 0.0003, train acc: 1.0000, val loss: 5.4631, val acc: 0.1716\n",
      "Epoch 48/50, train loss: 0.0003, train acc: 1.0000, val loss: 5.4709, val acc: 0.1716\n",
      "Epoch 49/50, train loss: 0.0003, train acc: 1.0000, val loss: 5.4786, val acc: 0.1719\n",
      "Epoch 50/50, train loss: 0.0002, train acc: 1.0000, val loss: 5.4864, val acc: 0.1720\n",
      "Epoch 1/50, train loss: 1.0256, train acc: 0.8569, val loss: 30.5182, val acc: 0.0371\n",
      "Epoch 2/50, train loss: 2.4804, train acc: 0.3773, val loss: 4.5914, val acc: 0.1403\n",
      "Epoch 3/50, train loss: 0.5909, train acc: 0.8525, val loss: 5.0308, val acc: 0.1436\n",
      "Epoch 4/50, train loss: 0.1598, train acc: 0.9677, val loss: 5.0467, val acc: 0.1570\n",
      "Epoch 5/50, train loss: 0.0369, train acc: 0.9972, val loss: 4.8648, val acc: 0.1809\n",
      "Epoch 6/50, train loss: 0.0097, train acc: 0.9996, val loss: 4.9052, val acc: 0.1838\n",
      "Epoch 7/50, train loss: 0.0038, train acc: 1.0000, val loss: 4.9168, val acc: 0.1834\n",
      "Epoch 8/50, train loss: 0.0028, train acc: 1.0000, val loss: 4.9394, val acc: 0.1832\n",
      "Epoch 9/50, train loss: 0.0024, train acc: 1.0000, val loss: 4.9595, val acc: 0.1830\n",
      "Epoch 10/50, train loss: 0.0021, train acc: 1.0000, val loss: 4.9784, val acc: 0.1833\n",
      "Epoch 11/50, train loss: 0.0019, train acc: 1.0000, val loss: 4.9962, val acc: 0.1831\n",
      "Epoch 12/50, train loss: 0.0017, train acc: 1.0000, val loss: 5.0129, val acc: 0.1828\n",
      "Epoch 13/50, train loss: 0.0015, train acc: 1.0000, val loss: 5.0289, val acc: 0.1834\n",
      "Epoch 14/50, train loss: 0.0014, train acc: 1.0000, val loss: 5.0442, val acc: 0.1834\n",
      "Epoch 15/50, train loss: 0.0013, train acc: 1.0000, val loss: 5.0589, val acc: 0.1837\n",
      "Epoch 16/50, train loss: 0.0012, train acc: 1.0000, val loss: 5.0730, val acc: 0.1844\n",
      "Epoch 17/50, train loss: 0.0011, train acc: 1.0000, val loss: 5.0868, val acc: 0.1843\n",
      "Epoch 18/50, train loss: 0.0010, train acc: 1.0000, val loss: 5.1001, val acc: 0.1843\n",
      "Epoch 19/50, train loss: 0.0009, train acc: 1.0000, val loss: 5.1131, val acc: 0.1842\n",
      "Epoch 20/50, train loss: 0.0009, train acc: 1.0000, val loss: 5.1257, val acc: 0.1839\n",
      "Epoch 21/50, train loss: 0.0008, train acc: 1.0000, val loss: 5.1382, val acc: 0.1835\n",
      "Epoch 22/50, train loss: 0.0008, train acc: 1.0000, val loss: 5.1502, val acc: 0.1834\n",
      "Epoch 23/50, train loss: 0.0007, train acc: 1.0000, val loss: 5.1623, val acc: 0.1831\n",
      "Epoch 24/50, train loss: 0.0007, train acc: 1.0000, val loss: 5.1740, val acc: 0.1831\n",
      "Epoch 25/50, train loss: 0.0006, train acc: 1.0000, val loss: 5.1855, val acc: 0.1830\n",
      "Epoch 26/50, train loss: 0.0006, train acc: 1.0000, val loss: 5.1969, val acc: 0.1832\n",
      "Epoch 27/50, train loss: 0.0006, train acc: 1.0000, val loss: 5.2081, val acc: 0.1831\n",
      "Epoch 28/50, train loss: 0.0005, train acc: 1.0000, val loss: 5.2191, val acc: 0.1827\n",
      "Epoch 29/50, train loss: 0.0005, train acc: 1.0000, val loss: 5.2301, val acc: 0.1824\n",
      "Epoch 30/50, train loss: 0.0005, train acc: 1.0000, val loss: 5.2409, val acc: 0.1823\n",
      "Epoch 31/50, train loss: 0.0004, train acc: 1.0000, val loss: 5.2515, val acc: 0.1827\n",
      "Epoch 32/50, train loss: 0.0004, train acc: 1.0000, val loss: 5.2622, val acc: 0.1829\n",
      "Epoch 33/50, train loss: 0.0004, train acc: 1.0000, val loss: 5.2727, val acc: 0.1828\n",
      "Epoch 34/50, train loss: 0.0004, train acc: 1.0000, val loss: 5.2830, val acc: 0.1830\n",
      "Epoch 35/50, train loss: 0.0004, train acc: 1.0000, val loss: 5.2934, val acc: 0.1829\n",
      "Epoch 36/50, train loss: 0.0003, train acc: 1.0000, val loss: 5.3036, val acc: 0.1830\n",
      "Epoch 37/50, train loss: 0.0003, train acc: 1.0000, val loss: 5.3139, val acc: 0.1832\n",
      "Epoch 38/50, train loss: 0.0003, train acc: 1.0000, val loss: 5.3240, val acc: 0.1834\n",
      "Epoch 39/50, train loss: 0.0003, train acc: 1.0000, val loss: 5.3341, val acc: 0.1836\n",
      "Epoch 40/50, train loss: 0.0003, train acc: 1.0000, val loss: 5.3441, val acc: 0.1836\n",
      "Epoch 41/50, train loss: 0.0003, train acc: 1.0000, val loss: 5.3540, val acc: 0.1837\n",
      "Epoch 42/50, train loss: 0.0002, train acc: 1.0000, val loss: 5.3639, val acc: 0.1838\n",
      "Epoch 43/50, train loss: 0.0002, train acc: 1.0000, val loss: 5.3737, val acc: 0.1837\n",
      "Epoch 44/50, train loss: 0.0002, train acc: 1.0000, val loss: 5.3835, val acc: 0.1834\n",
      "Epoch 45/50, train loss: 0.0002, train acc: 1.0000, val loss: 5.3934, val acc: 0.1833\n",
      "Epoch 46/50, train loss: 0.0002, train acc: 1.0000, val loss: 5.4031, val acc: 0.1833\n",
      "Epoch 47/50, train loss: 0.0002, train acc: 1.0000, val loss: 5.4127, val acc: 0.1831\n",
      "Epoch 48/50, train loss: 0.0002, train acc: 1.0000, val loss: 5.4223, val acc: 0.1832\n",
      "Epoch 49/50, train loss: 0.0002, train acc: 1.0000, val loss: 5.4319, val acc: 0.1831\n",
      "Epoch 50/50, train loss: 0.0002, train acc: 1.0000, val loss: 5.4415, val acc: 0.1833\n",
      "Epoch 1/50, train loss: 0.8267, train acc: 0.8840, val loss: 10.8847, val acc: 0.0505\n",
      "Epoch 2/50, train loss: 1.4565, train acc: 0.6195, val loss: 5.1000, val acc: 0.1353\n",
      "Epoch 3/50, train loss: 0.2100, train acc: 0.9540, val loss: 5.1360, val acc: 0.1591\n",
      "Epoch 4/50, train loss: 0.0449, train acc: 0.9933, val loss: 4.9089, val acc: 0.1833\n",
      "Epoch 5/50, train loss: 0.0084, train acc: 0.9997, val loss: 4.8598, val acc: 0.1911\n",
      "Epoch 6/50, train loss: 0.0036, train acc: 0.9998, val loss: 4.8749, val acc: 0.1900\n",
      "Epoch 7/50, train loss: 0.0023, train acc: 1.0000, val loss: 4.8993, val acc: 0.1921\n",
      "Epoch 8/50, train loss: 0.0019, train acc: 1.0000, val loss: 4.9249, val acc: 0.1920\n",
      "Epoch 9/50, train loss: 0.0016, train acc: 1.0000, val loss: 4.9472, val acc: 0.1911\n",
      "Epoch 10/50, train loss: 0.0014, train acc: 1.0000, val loss: 4.9678, val acc: 0.1912\n",
      "Epoch 11/50, train loss: 0.0013, train acc: 1.0000, val loss: 4.9869, val acc: 0.1916\n",
      "Epoch 12/50, train loss: 0.0011, train acc: 1.0000, val loss: 5.0053, val acc: 0.1914\n",
      "Epoch 13/50, train loss: 0.0010, train acc: 1.0000, val loss: 5.0226, val acc: 0.1914\n",
      "Epoch 14/50, train loss: 0.0009, train acc: 1.0000, val loss: 5.0394, val acc: 0.1910\n",
      "Epoch 15/50, train loss: 0.0009, train acc: 1.0000, val loss: 5.0554, val acc: 0.1908\n",
      "Epoch 16/50, train loss: 0.0008, train acc: 1.0000, val loss: 5.0710, val acc: 0.1911\n",
      "Epoch 17/50, train loss: 0.0007, train acc: 1.0000, val loss: 5.0861, val acc: 0.1915\n",
      "Epoch 18/50, train loss: 0.0007, train acc: 1.0000, val loss: 5.1007, val acc: 0.1918\n",
      "Epoch 19/50, train loss: 0.0006, train acc: 1.0000, val loss: 5.1151, val acc: 0.1914\n",
      "Epoch 20/50, train loss: 0.0006, train acc: 1.0000, val loss: 5.1291, val acc: 0.1915\n",
      "Epoch 21/50, train loss: 0.0005, train acc: 1.0000, val loss: 5.1428, val acc: 0.1909\n",
      "Epoch 22/50, train loss: 0.0005, train acc: 1.0000, val loss: 5.1564, val acc: 0.1910\n",
      "Epoch 23/50, train loss: 0.0005, train acc: 1.0000, val loss: 5.1696, val acc: 0.1909\n",
      "Epoch 24/50, train loss: 0.0004, train acc: 1.0000, val loss: 5.1828, val acc: 0.1908\n",
      "Epoch 25/50, train loss: 0.0004, train acc: 1.0000, val loss: 5.1957, val acc: 0.1907\n",
      "Epoch 26/50, train loss: 0.0004, train acc: 1.0000, val loss: 5.2085, val acc: 0.1906\n",
      "Epoch 27/50, train loss: 0.0004, train acc: 1.0000, val loss: 5.2211, val acc: 0.1907\n",
      "Epoch 28/50, train loss: 0.0003, train acc: 1.0000, val loss: 5.2336, val acc: 0.1905\n",
      "Epoch 29/50, train loss: 0.0003, train acc: 1.0000, val loss: 5.2460, val acc: 0.1908\n",
      "Epoch 30/50, train loss: 0.0003, train acc: 1.0000, val loss: 5.2583, val acc: 0.1907\n",
      "Epoch 31/50, train loss: 0.0003, train acc: 1.0000, val loss: 5.2704, val acc: 0.1905\n",
      "Epoch 32/50, train loss: 0.0003, train acc: 1.0000, val loss: 5.2826, val acc: 0.1901\n",
      "Epoch 33/50, train loss: 0.0003, train acc: 1.0000, val loss: 5.2945, val acc: 0.1897\n",
      "Epoch 34/50, train loss: 0.0002, train acc: 1.0000, val loss: 5.3064, val acc: 0.1901\n",
      "Epoch 35/50, train loss: 0.0002, train acc: 1.0000, val loss: 5.3183, val acc: 0.1903\n",
      "Epoch 36/50, train loss: 0.0002, train acc: 1.0000, val loss: 5.3301, val acc: 0.1903\n",
      "Epoch 37/50, train loss: 0.0002, train acc: 1.0000, val loss: 5.3418, val acc: 0.1900\n",
      "Epoch 38/50, train loss: 0.0002, train acc: 1.0000, val loss: 5.3534, val acc: 0.1898\n",
      "Epoch 39/50, train loss: 0.0002, train acc: 1.0000, val loss: 5.3651, val acc: 0.1897\n",
      "Epoch 40/50, train loss: 0.0002, train acc: 1.0000, val loss: 5.3766, val acc: 0.1896\n",
      "Epoch 41/50, train loss: 0.0002, train acc: 1.0000, val loss: 5.3881, val acc: 0.1900\n",
      "Epoch 42/50, train loss: 0.0002, train acc: 1.0000, val loss: 5.3996, val acc: 0.1900\n",
      "Epoch 43/50, train loss: 0.0001, train acc: 1.0000, val loss: 5.4110, val acc: 0.1902\n",
      "Epoch 44/50, train loss: 0.0001, train acc: 1.0000, val loss: 5.4224, val acc: 0.1909\n",
      "Epoch 45/50, train loss: 0.0001, train acc: 1.0000, val loss: 5.4338, val acc: 0.1908\n",
      "Epoch 46/50, train loss: 0.0001, train acc: 1.0000, val loss: 5.4451, val acc: 0.1912\n",
      "Epoch 47/50, train loss: 0.0001, train acc: 1.0000, val loss: 5.4565, val acc: 0.1906\n",
      "Epoch 48/50, train loss: 0.0001, train acc: 1.0000, val loss: 5.4677, val acc: 0.1902\n",
      "Epoch 49/50, train loss: 0.0001, train acc: 1.0000, val loss: 5.4791, val acc: 0.1899\n",
      "Epoch 50/50, train loss: 0.0001, train acc: 1.0000, val loss: 5.4903, val acc: 0.1899\n",
      "Epoch 1/50, train loss: 0.7275, train acc: 0.8983, val loss: 7.7979, val acc: 0.0863\n",
      "Epoch 2/50, train loss: 1.0963, train acc: 0.7140, val loss: 5.5892, val acc: 0.1417\n",
      "Epoch 3/50, train loss: 0.1744, train acc: 0.9601, val loss: 4.9937, val acc: 0.1672\n",
      "Epoch 4/50, train loss: 0.0350, train acc: 0.9955, val loss: 4.8921, val acc: 0.1845\n",
      "Epoch 5/50, train loss: 0.0089, train acc: 0.9993, val loss: 4.8908, val acc: 0.1900\n",
      "Epoch 6/50, train loss: 0.0033, train acc: 0.9999, val loss: 4.8926, val acc: 0.1917\n",
      "Epoch 7/50, train loss: 0.0020, train acc: 1.0000, val loss: 4.9114, val acc: 0.1922\n",
      "Epoch 8/50, train loss: 0.0016, train acc: 1.0000, val loss: 4.9334, val acc: 0.1922\n",
      "Epoch 9/50, train loss: 0.0014, train acc: 1.0000, val loss: 4.9539, val acc: 0.1919\n",
      "Epoch 10/50, train loss: 0.0012, train acc: 1.0000, val loss: 4.9732, val acc: 0.1921\n",
      "Epoch 11/50, train loss: 0.0011, train acc: 1.0000, val loss: 4.9914, val acc: 0.1916\n",
      "Epoch 12/50, train loss: 0.0009, train acc: 1.0000, val loss: 5.0089, val acc: 0.1919\n",
      "Epoch 13/50, train loss: 0.0009, train acc: 1.0000, val loss: 5.0256, val acc: 0.1919\n",
      "Epoch 14/50, train loss: 0.0008, train acc: 1.0000, val loss: 5.0417, val acc: 0.1909\n",
      "Epoch 15/50, train loss: 0.0007, train acc: 1.0000, val loss: 5.0572, val acc: 0.1911\n",
      "Epoch 16/50, train loss: 0.0006, train acc: 1.0000, val loss: 5.0724, val acc: 0.1913\n",
      "Epoch 17/50, train loss: 0.0006, train acc: 1.0000, val loss: 5.0874, val acc: 0.1913\n",
      "Epoch 18/50, train loss: 0.0005, train acc: 1.0000, val loss: 5.1019, val acc: 0.1916\n",
      "Epoch 19/50, train loss: 0.0005, train acc: 1.0000, val loss: 5.1162, val acc: 0.1917\n",
      "Epoch 20/50, train loss: 0.0005, train acc: 1.0000, val loss: 5.1302, val acc: 0.1917\n",
      "Epoch 21/50, train loss: 0.0004, train acc: 1.0000, val loss: 5.1439, val acc: 0.1921\n",
      "Epoch 22/50, train loss: 0.0004, train acc: 1.0000, val loss: 5.1575, val acc: 0.1916\n",
      "Epoch 23/50, train loss: 0.0004, train acc: 1.0000, val loss: 5.1709, val acc: 0.1916\n",
      "Epoch 24/50, train loss: 0.0004, train acc: 1.0000, val loss: 5.1843, val acc: 0.1916\n",
      "Epoch 25/50, train loss: 0.0003, train acc: 1.0000, val loss: 5.1975, val acc: 0.1918\n",
      "Epoch 26/50, train loss: 0.0003, train acc: 1.0000, val loss: 5.2106, val acc: 0.1917\n",
      "Epoch 27/50, train loss: 0.0003, train acc: 1.0000, val loss: 5.2236, val acc: 0.1920\n",
      "Epoch 28/50, train loss: 0.0003, train acc: 1.0000, val loss: 5.2365, val acc: 0.1920\n",
      "Epoch 29/50, train loss: 0.0003, train acc: 1.0000, val loss: 5.2493, val acc: 0.1923\n",
      "Epoch 30/50, train loss: 0.0002, train acc: 1.0000, val loss: 5.2619, val acc: 0.1921\n",
      "Epoch 31/50, train loss: 0.0002, train acc: 1.0000, val loss: 5.2746, val acc: 0.1920\n",
      "Epoch 32/50, train loss: 0.0002, train acc: 1.0000, val loss: 5.2873, val acc: 0.1919\n",
      "Epoch 33/50, train loss: 0.0002, train acc: 1.0000, val loss: 5.2999, val acc: 0.1916\n",
      "Epoch 34/50, train loss: 0.0002, train acc: 1.0000, val loss: 5.3124, val acc: 0.1917\n",
      "Epoch 35/50, train loss: 0.0002, train acc: 1.0000, val loss: 5.3249, val acc: 0.1923\n",
      "Epoch 36/50, train loss: 0.0002, train acc: 1.0000, val loss: 5.3373, val acc: 0.1925\n",
      "Epoch 37/50, train loss: 0.0002, train acc: 1.0000, val loss: 5.3498, val acc: 0.1923\n",
      "Epoch 38/50, train loss: 0.0001, train acc: 1.0000, val loss: 5.3622, val acc: 0.1922\n",
      "Epoch 39/50, train loss: 0.0001, train acc: 1.0000, val loss: 5.3747, val acc: 0.1918\n",
      "Epoch 40/50, train loss: 0.0001, train acc: 1.0000, val loss: 5.3871, val acc: 0.1918\n",
      "Epoch 41/50, train loss: 0.0001, train acc: 1.0000, val loss: 5.3995, val acc: 0.1918\n",
      "Epoch 42/50, train loss: 0.0001, train acc: 1.0000, val loss: 5.4120, val acc: 0.1918\n",
      "Epoch 43/50, train loss: 0.0001, train acc: 1.0000, val loss: 5.4243, val acc: 0.1917\n",
      "Epoch 44/50, train loss: 0.0001, train acc: 1.0000, val loss: 5.4367, val acc: 0.1916\n",
      "Epoch 45/50, train loss: 0.0001, train acc: 1.0000, val loss: 5.4491, val acc: 0.1913\n",
      "Epoch 46/50, train loss: 0.0001, train acc: 1.0000, val loss: 5.4615, val acc: 0.1915\n",
      "Epoch 47/50, train loss: 0.0001, train acc: 1.0000, val loss: 5.4738, val acc: 0.1911\n",
      "Epoch 48/50, train loss: 0.0001, train acc: 1.0000, val loss: 5.4862, val acc: 0.1910\n",
      "Epoch 49/50, train loss: 0.0001, train acc: 1.0000, val loss: 5.4986, val acc: 0.1908\n",
      "Epoch 50/50, train loss: 0.0001, train acc: 1.0000, val loss: 5.5111, val acc: 0.1910\n",
      "Epoch 1/50, train loss: 0.6130, train acc: 0.9095, val loss: 7.9607, val acc: 0.0925\n",
      "Epoch 2/50, train loss: 0.7838, train acc: 0.7989, val loss: 5.0197, val acc: 0.1749\n",
      "Epoch 3/50, train loss: 0.1130, train acc: 0.9759, val loss: 5.0550, val acc: 0.1828\n",
      "Epoch 4/50, train loss: 0.0236, train acc: 0.9956, val loss: 4.9531, val acc: 0.1951\n",
      "Epoch 5/50, train loss: 0.0071, train acc: 0.9992, val loss: 4.9976, val acc: 0.2029\n",
      "Epoch 6/50, train loss: 0.0025, train acc: 0.9998, val loss: 5.0131, val acc: 0.2042\n",
      "Epoch 7/50, train loss: 0.0013, train acc: 1.0000, val loss: 5.0298, val acc: 0.2051\n",
      "Epoch 8/50, train loss: 0.0011, train acc: 1.0000, val loss: 5.0495, val acc: 0.2050\n",
      "Epoch 9/50, train loss: 0.0009, train acc: 1.0000, val loss: 5.0681, val acc: 0.2054\n",
      "Epoch 10/50, train loss: 0.0008, train acc: 1.0000, val loss: 5.0859, val acc: 0.2059\n",
      "Epoch 11/50, train loss: 0.0007, train acc: 1.0000, val loss: 5.1030, val acc: 0.2062\n",
      "Epoch 12/50, train loss: 0.0006, train acc: 1.0000, val loss: 5.1194, val acc: 0.2066\n",
      "Epoch 13/50, train loss: 0.0006, train acc: 1.0000, val loss: 5.1353, val acc: 0.2066\n",
      "Epoch 14/50, train loss: 0.0005, train acc: 1.0000, val loss: 5.1508, val acc: 0.2065\n",
      "Epoch 15/50, train loss: 0.0005, train acc: 1.0000, val loss: 5.1658, val acc: 0.2063\n",
      "Epoch 16/50, train loss: 0.0004, train acc: 1.0000, val loss: 5.1805, val acc: 0.2064\n",
      "Epoch 17/50, train loss: 0.0004, train acc: 1.0000, val loss: 5.1949, val acc: 0.2065\n",
      "Epoch 18/50, train loss: 0.0004, train acc: 1.0000, val loss: 5.2090, val acc: 0.2066\n",
      "Epoch 19/50, train loss: 0.0003, train acc: 1.0000, val loss: 5.2230, val acc: 0.2068\n",
      "Epoch 20/50, train loss: 0.0003, train acc: 1.0000, val loss: 5.2369, val acc: 0.2069\n",
      "Epoch 21/50, train loss: 0.0003, train acc: 1.0000, val loss: 5.2506, val acc: 0.2067\n",
      "Epoch 22/50, train loss: 0.0003, train acc: 1.0000, val loss: 5.2641, val acc: 0.2067\n",
      "Epoch 23/50, train loss: 0.0002, train acc: 1.0000, val loss: 5.2776, val acc: 0.2067\n",
      "Epoch 24/50, train loss: 0.0002, train acc: 1.0000, val loss: 5.2910, val acc: 0.2066\n",
      "Epoch 25/50, train loss: 0.0002, train acc: 1.0000, val loss: 5.3041, val acc: 0.2067\n",
      "Epoch 26/50, train loss: 0.0002, train acc: 1.0000, val loss: 5.3173, val acc: 0.2072\n",
      "Epoch 27/50, train loss: 0.0002, train acc: 1.0000, val loss: 5.3304, val acc: 0.2073\n",
      "Epoch 28/50, train loss: 0.0002, train acc: 1.0000, val loss: 5.3435, val acc: 0.2076\n",
      "Epoch 29/50, train loss: 0.0002, train acc: 1.0000, val loss: 5.3565, val acc: 0.2076\n",
      "Epoch 30/50, train loss: 0.0002, train acc: 1.0000, val loss: 5.3695, val acc: 0.2076\n",
      "Epoch 31/50, train loss: 0.0001, train acc: 1.0000, val loss: 5.3825, val acc: 0.2078\n",
      "Epoch 32/50, train loss: 0.0001, train acc: 1.0000, val loss: 5.3955, val acc: 0.2078\n",
      "Epoch 33/50, train loss: 0.0001, train acc: 1.0000, val loss: 5.4084, val acc: 0.2081\n",
      "Epoch 34/50, train loss: 0.0001, train acc: 1.0000, val loss: 5.4212, val acc: 0.2079\n",
      "Epoch 35/50, train loss: 0.0001, train acc: 1.0000, val loss: 5.4341, val acc: 0.2079\n",
      "Epoch 36/50, train loss: 0.0001, train acc: 1.0000, val loss: 5.4470, val acc: 0.2081\n",
      "Epoch 37/50, train loss: 0.0001, train acc: 1.0000, val loss: 5.4599, val acc: 0.2082\n",
      "Epoch 38/50, train loss: 0.0001, train acc: 1.0000, val loss: 5.4729, val acc: 0.2083\n",
      "Epoch 39/50, train loss: 0.0001, train acc: 1.0000, val loss: 5.4857, val acc: 0.2082\n",
      "Epoch 40/50, train loss: 0.0001, train acc: 1.0000, val loss: 5.4987, val acc: 0.2079\n",
      "Epoch 41/50, train loss: 0.0001, train acc: 1.0000, val loss: 5.5115, val acc: 0.2078\n",
      "Epoch 42/50, train loss: 0.0001, train acc: 1.0000, val loss: 5.5246, val acc: 0.2080\n",
      "Epoch 43/50, train loss: 0.0001, train acc: 1.0000, val loss: 5.5374, val acc: 0.2079\n",
      "Epoch 44/50, train loss: 0.0001, train acc: 1.0000, val loss: 5.5504, val acc: 0.2079\n",
      "Epoch 45/50, train loss: 0.0001, train acc: 1.0000, val loss: 5.5635, val acc: 0.2081\n",
      "Epoch 46/50, train loss: 0.0001, train acc: 1.0000, val loss: 5.5764, val acc: 0.2082\n",
      "Epoch 47/50, train loss: 0.0000, train acc: 1.0000, val loss: 5.5894, val acc: 0.2084\n",
      "Epoch 48/50, train loss: 0.0000, train acc: 1.0000, val loss: 5.6025, val acc: 0.2085\n",
      "Epoch 49/50, train loss: 0.0000, train acc: 1.0000, val loss: 5.6156, val acc: 0.2086\n",
      "Epoch 50/50, train loss: 0.0000, train acc: 1.0000, val loss: 5.6286, val acc: 0.2085\n",
      "Epoch 1/50, train loss: 0.6669, train acc: 0.9180, val loss: 8.4706, val acc: 0.0867\n",
      "Epoch 2/50, train loss: 0.6234, train acc: 0.8358, val loss: 5.4425, val acc: 0.1743\n",
      "Epoch 3/50, train loss: 0.0780, train acc: 0.9826, val loss: 4.8829, val acc: 0.1930\n",
      "Epoch 4/50, train loss: 0.0106, train acc: 0.9991, val loss: 4.7818, val acc: 0.2047\n",
      "Epoch 5/50, train loss: 0.0028, train acc: 1.0000, val loss: 4.8128, val acc: 0.2032\n",
      "Epoch 6/50, train loss: 0.0017, train acc: 1.0000, val loss: 4.8399, val acc: 0.2039\n",
      "Epoch 7/50, train loss: 0.0013, train acc: 1.0000, val loss: 4.8664, val acc: 0.2045\n",
      "Epoch 8/50, train loss: 0.0011, train acc: 1.0000, val loss: 4.8908, val acc: 0.2052\n",
      "Epoch 9/50, train loss: 0.0010, train acc: 1.0000, val loss: 4.9134, val acc: 0.2043\n",
      "Epoch 10/50, train loss: 0.0008, train acc: 1.0000, val loss: 4.9347, val acc: 0.2048\n",
      "Epoch 11/50, train loss: 0.0007, train acc: 1.0000, val loss: 4.9549, val acc: 0.2057\n",
      "Epoch 12/50, train loss: 0.0007, train acc: 1.0000, val loss: 4.9742, val acc: 0.2055\n",
      "Epoch 13/50, train loss: 0.0006, train acc: 1.0000, val loss: 4.9929, val acc: 0.2053\n",
      "Epoch 14/50, train loss: 0.0005, train acc: 1.0000, val loss: 5.0110, val acc: 0.2051\n",
      "Epoch 15/50, train loss: 0.0005, train acc: 1.0000, val loss: 5.0286, val acc: 0.2053\n",
      "Epoch 16/50, train loss: 0.0004, train acc: 1.0000, val loss: 5.0457, val acc: 0.2053\n",
      "Epoch 17/50, train loss: 0.0004, train acc: 1.0000, val loss: 5.0626, val acc: 0.2057\n",
      "Epoch 18/50, train loss: 0.0004, train acc: 1.0000, val loss: 5.0791, val acc: 0.2053\n",
      "Epoch 19/50, train loss: 0.0003, train acc: 1.0000, val loss: 5.0953, val acc: 0.2053\n",
      "Epoch 20/50, train loss: 0.0003, train acc: 1.0000, val loss: 5.1115, val acc: 0.2056\n",
      "Epoch 21/50, train loss: 0.0003, train acc: 1.0000, val loss: 5.1274, val acc: 0.2055\n",
      "Epoch 22/50, train loss: 0.0003, train acc: 1.0000, val loss: 5.1431, val acc: 0.2057\n",
      "Epoch 23/50, train loss: 0.0002, train acc: 1.0000, val loss: 5.1588, val acc: 0.2055\n",
      "Epoch 24/50, train loss: 0.0002, train acc: 1.0000, val loss: 5.1743, val acc: 0.2052\n",
      "Epoch 25/50, train loss: 0.0002, train acc: 1.0000, val loss: 5.1897, val acc: 0.2052\n",
      "Epoch 26/50, train loss: 0.0002, train acc: 1.0000, val loss: 5.2050, val acc: 0.2053\n",
      "Epoch 27/50, train loss: 0.0002, train acc: 1.0000, val loss: 5.2203, val acc: 0.2055\n",
      "Epoch 28/50, train loss: 0.0002, train acc: 1.0000, val loss: 5.2355, val acc: 0.2049\n",
      "Epoch 29/50, train loss: 0.0002, train acc: 1.0000, val loss: 5.2506, val acc: 0.2049\n",
      "Epoch 30/50, train loss: 0.0001, train acc: 1.0000, val loss: 5.2657, val acc: 0.2049\n",
      "Epoch 31/50, train loss: 0.0001, train acc: 1.0000, val loss: 5.2809, val acc: 0.2049\n",
      "Epoch 32/50, train loss: 0.0001, train acc: 1.0000, val loss: 5.2959, val acc: 0.2048\n",
      "Epoch 33/50, train loss: 0.0001, train acc: 1.0000, val loss: 5.3109, val acc: 0.2045\n",
      "Epoch 34/50, train loss: 0.0001, train acc: 1.0000, val loss: 5.3259, val acc: 0.2044\n",
      "Epoch 35/50, train loss: 0.0001, train acc: 1.0000, val loss: 5.3408, val acc: 0.2044\n",
      "Epoch 36/50, train loss: 0.0001, train acc: 1.0000, val loss: 5.3559, val acc: 0.2040\n",
      "Epoch 37/50, train loss: 0.0001, train acc: 1.0000, val loss: 5.3708, val acc: 0.2035\n",
      "Epoch 38/50, train loss: 0.0001, train acc: 1.0000, val loss: 5.3859, val acc: 0.2034\n",
      "Epoch 39/50, train loss: 0.0001, train acc: 1.0000, val loss: 5.4009, val acc: 0.2034\n",
      "Epoch 40/50, train loss: 0.0001, train acc: 1.0000, val loss: 5.4159, val acc: 0.2032\n",
      "Epoch 41/50, train loss: 0.0001, train acc: 1.0000, val loss: 5.4309, val acc: 0.2034\n",
      "Epoch 42/50, train loss: 0.0001, train acc: 1.0000, val loss: 5.4459, val acc: 0.2033\n",
      "Epoch 43/50, train loss: 0.0001, train acc: 1.0000, val loss: 5.4610, val acc: 0.2034\n",
      "Epoch 44/50, train loss: 0.0000, train acc: 1.0000, val loss: 5.4761, val acc: 0.2034\n",
      "Epoch 45/50, train loss: 0.0000, train acc: 1.0000, val loss: 5.4912, val acc: 0.2030\n",
      "Epoch 46/50, train loss: 0.0000, train acc: 1.0000, val loss: 5.5062, val acc: 0.2032\n",
      "Epoch 47/50, train loss: 0.0000, train acc: 1.0000, val loss: 5.5214, val acc: 0.2035\n",
      "Epoch 48/50, train loss: 0.0000, train acc: 1.0000, val loss: 5.5365, val acc: 0.2035\n",
      "Epoch 49/50, train loss: 0.0000, train acc: 1.0000, val loss: 5.5517, val acc: 0.2037\n",
      "Epoch 50/50, train loss: 0.0000, train acc: 1.0000, val loss: 5.5668, val acc: 0.2036\n"
     ]
    }
   ],
   "source": [
    "## Repeat until budget is exhausted\n",
    "\n",
    "acc_train_list = []\n",
    "acc_val_list = []\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "f1_list = []\n",
    "\n",
    "\n",
    "for n in range(n_iter):\n",
    "\n",
    "    #Train classifier on all the examples selected so far\n",
    "\n",
    "    train_acc, val_acc, precision, recall, f1 = training(train_loader, label_num, ms)\n",
    "    \n",
    "    wandb.log({\"custom_step\": n+1,\n",
    "               \"test/accuracy\": val_acc,\n",
    "                   \"test_precision\":precision,\n",
    "                   \"test_recall\":recall,\n",
    "                   \"test_f1\": f1})\n",
    "\n",
    "    #Add accuracies of training round\n",
    "    acc_train_list.append(train_acc)\n",
    "    acc_val_list.append(val_acc)\n",
    "    precision_list.append(precision)\n",
    "    recall_list.append(recall)\n",
    "    f1_list.append(f1)\n",
    "\n",
    "    # Create dictionary that contains informative factor for each unlabeled datapoint\n",
    "    uncert_outputs = {}\n",
    "#     cos_outputs_0 = {}\n",
    "#     cos_outputs_1 = {}\n",
    "#     cos_outputs_2 = {}\n",
    "#     cos_outputs_3 = {}\n",
    "#     cos_outputs_4 = {}\n",
    "#     cos_outputs_5 = {}\n",
    "#     cos_outputs_6 = {}\n",
    "#     cos_outputs_7 = {}\n",
    "#     cos_outputs_8 = {}\n",
    "#     cos_outputs_9 = {}\n",
    "#     cos_outputs_10 = {}\n",
    "    for x in range(0,label_num):\n",
    "        globals()[f'cos_outputs_{x}'] = {}\n",
    "    \n",
    "    # Compute informativeness factor for each unlabeled datapoint per batch\n",
    "    for idx,(inputs,_) in enumerate(unlabeled_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        model.eval()\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Add maximum separation\n",
    "        #if ms == True:\n",
    "            #outputs = torch.mm(outputs, prototypes)\n",
    "\n",
    "        # use softmax to get probability distribuition for each datapoint in batch\n",
    "        probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        # sort probabilities\n",
    "        prob,_ = torch.sort(probs, descending=True)\n",
    "        # compute difference between highest and second highest probability (= informative factor)\n",
    "        diff = prob.data[:,0] - prob.data[:,1]\n",
    "        #for prototype in prototypes.t():\n",
    "        # Select the k closest to the prototypes\n",
    "                #output_cos = torch.nn.functional.cosine_similarity(outputs_k.unsqueeze(1), prototype,dim=-1)\n",
    "        idy = 0\n",
    "        for x in diff:\n",
    "            index = (idx*64) + idy\n",
    "            # Save index of each datapoint with given informative factor\n",
    "            uncert_outputs[index] = x\n",
    "            idy += 1\n",
    "\n",
    "    # Select the k most informative unlabeled datapoints\n",
    "    k_dataset = sorted(uncert_outputs, key=uncert_outputs.get, reverse=True)[:10000]\n",
    "    # Select all other unlabeled datapoints\n",
    "    remain_set = sorted(uncert_outputs, key=uncert_outputs.get, reverse=True)[10000:]\n",
    "    unlabeled_loader = torch.utils.data.Subset(unlabeled_loader.dataset, remain_set)\n",
    "    \n",
    "    k_dataset = torch.utils.data.Subset(unlabeled_loader.dataset, k_dataset)\n",
    "    k_loader = torch.utils.data.DataLoader(k_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
    "    \n",
    "    del uncert_outputs\n",
    "    \n",
    "    #k_data = torch.utils.data.DataLoader(k_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
    "    for idx,(inputs,_) in enumerate(k_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        model.eval()\n",
    "        outputs_k = model(inputs)\n",
    "        # Add maximum separation\n",
    "        if ms == True:\n",
    "            outputs_k = torch.mm(outputs_k, prototypes)\n",
    "        for idp, (prototype) in enumerate(prototypes):\n",
    "            # Select the k closest to the prototypes\n",
    "            output_cos = torch.nn.functional.cosine_similarity(outputs_k, prototype,dim=1)\n",
    "            idy = 0\n",
    "            for x in output_cos:\n",
    "                index = (idx*64) + idy\n",
    "                # Save index of each datapoint with given informative factor\n",
    "                globals()[f'cos_outputs_{idp}'][index] = x.cpu().detach().numpy()\n",
    "                idy += 1\n",
    "                    \n",
    "    k_dataset = []\n",
    "    \n",
    "    for idp, (prototype) in enumerate(prototypes):\n",
    "        # Add 100 closest to prototype to k dataset\n",
    "        k_dataset += (sorted(globals()[f'cos_outputs_{idp}'], key=globals()[f'cos_outputs_{idp}'].get)[:10])\n",
    "        if idp < 8:\n",
    "            next_dict = idp + 1\n",
    "            globals()[f'cos_outputs_{next_dict}'] = {k: v for k, v in globals()[f'cos_outputs_{next_dict}'].items() if k not in k_dataset}\n",
    "    \n",
    "    k_dataset = set(k_dataset)\n",
    "    index_list = set(range(0,10000))\n",
    "    rest_list = list(index_list - k_dataset)\n",
    "    k_dataset = list(k_dataset)\n",
    "    rest_set = torch.utils.data.Subset(k_loader.dataset, rest_list)\n",
    "\n",
    "    \n",
    "    # Obtain label of new k datapoints\n",
    "    k_dataset = torch.utils.data.Subset(k_loader.dataset, k_dataset)\n",
    "    k_dataset =  torch.utils.data.ConcatDataset([train_loader.dataset, k_dataset])\n",
    "    train_loader = torch.utils.data.DataLoader(k_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "    # Remove new k datapoints from unlabeled dataset\n",
    "    unlabeled_loader =  torch.utils.data.ConcatDataset([unlabeled_loader, rest_set])\n",
    "    unlabeled_loader = torch.utils.data.DataLoader(unlabeled_loader, batch_size=64, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
